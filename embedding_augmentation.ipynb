{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x == 1 else 0)\n",
    "df = df[['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column):\n",
    "    \"\"\"\n",
    "    Funkcija za predobradu teksta.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Pandas DataFrame koji sadr≈æi kolonu sa tekstom.\n",
    "    - text_column: Ime kolone koja sadr≈æi tekstualne podatke.\n",
    "    \n",
    "    Vraƒáa:\n",
    "    - df: Pandas DataFrame sa novom kolonom 'clean_text' koja sadr≈æi predobraƒëen tekst.\n",
    "    \"\"\"\n",
    "    # Kopiramo originalni DataFrame da ne bismo menjali originalne podatke\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Uklanjanje URL-ova i email adresa\n",
    "    df['clean_tweet'] = df[text_column].apply(lambda x: re.sub(r'http\\S+|www.\\S+|mailto:\\S+', '', x))\n",
    "    \n",
    "    # Uklanjanje HTML tagova\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "    \n",
    "    # Uklanjanje emotikona i specijalnih karaktera\n",
    "    def remove_emojis(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emotikoni\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # simobli i ikone\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport i simobli\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # zastave\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_emojis)\n",
    "    \n",
    "    # Uklanjanje specijalnih karaktera i interpunkcije\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^A-Za-z≈°ƒëƒçƒá≈æ≈†ƒêƒåƒÜ≈Ω ]+', ' ', x))\n",
    "    \n",
    "    # Pretvaranje u mala slova\n",
    "    df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "    \n",
    "    # Uklanjanje dijakritika\n",
    "    def remove_diacritics(text):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "        return text\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_diacritics)\n",
    "    \n",
    "    # Uklanjanje vi≈°estrukih razmaka\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
    "    \n",
    "    # Tokenizacija\n",
    "    df['tokens'] = df['clean_tweet'].apply(nltk.word_tokenize)\n",
    "    \n",
    "    # Uklanjanje stop-reƒçi\n",
    "    stop_words = set(stopwords.words('english'))  # Ako ima≈° stop-reƒçi za srpski, zameni ovde\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    \n",
    "    # Lematizacija\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # Spajanje tokena nazad u string\n",
    "    df['clean_tweet'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Uklanjanje nepotrebnih kolona\n",
    "    df = df.drop(columns=['tokens'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katarina.stanojkovic\\AppData\\Local\\Temp\\ipykernel_11304\\2996977020.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df['clean_tweet'] = df['clean_tweet'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_text(df, 'tweet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "from textattack.augmentation import Augmenter, BackTranslationAugmenter\n",
    "from textattack.transformations import WordInsertionMaskedLM, WordSwapWordNet\n",
    "from textattack.augmentation.recipes import CLAREAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from textattack.augmentation import (\n",
    "    EmbeddingAugmenter,\n",
    "    CLAREAugmenter\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "# Inicijalizacija TextAttack augmentera\n",
    "embedding_augmenter = EmbeddingAugmenter()\n",
    "clare_augmenter = CLAREAugmenter(model='distilroberta-base', tokenizer='distilroberta-base')\n",
    "\n",
    "# Definisanje wrapper funkcija\n",
    "def embedding_augment(text, **kwargs):\n",
    "    augmented_texts = embedding_augmenter.augment(text)\n",
    "    return random.choice(augmented_texts) if augmented_texts else text\n",
    "\n",
    "def clare_augment(text, **kwargs):\n",
    "    augmented_texts = clare_augmenter.augment(text)\n",
    "    return random.choice(augmented_texts) if augmented_texts else text\n",
    "\n",
    "# Lista TextAttack augmentera kao funkcija\n",
    "textattack_methods = [\n",
    "    clare_augment,\n",
    "    embedding_augment\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "def augment_dataset_parallel(df, text_column, method, num_augmented_instances, max_workers=4, **kwargs):\n",
    "    \"\"\"\n",
    "    Primeni izabranu metodu augmentacije na dataset koristeƒái paralelizaciju.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Originalni DataFrame.\n",
    "    - text_column: Naziv kolone sa tekstom.\n",
    "    - method: Funkcija metode augmentacije.\n",
    "    - num_augmented_instances: Broj instanci koje treba generisati.\n",
    "    - max_workers: Broj paralelnih radnika.\n",
    "    - **kwargs: Dodatni argumenti za metodu augmentacije.\n",
    "    \n",
    "    Vraƒáa:\n",
    "    - DataFrame sa augmentiranim podacima.\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    indices = df.index.tolist()\n",
    "    num_samples = len(indices)\n",
    "    \n",
    "    # Ako je broj instanci veƒái od broja dostupnih uzoraka, uzmi uzorke sa zamjenom\n",
    "    replace = num_augmented_instances > num_samples\n",
    "    sampled_indices = np.random.choice(indices, size=num_augmented_instances, replace=replace)\n",
    "    \n",
    "    def augment_text(idx):\n",
    "        original_text = df.loc[idx, text_column]\n",
    "        try:\n",
    "            augmented_text = method(original_text, **kwargs)\n",
    "            return augmented_text\n",
    "        except Exception as e:\n",
    "            print(f\"Gre≈°ka pri augmentaciji teksta na indeksu {idx}: {e}\")\n",
    "            return original_text  # Vraƒáa originalni tekst u sluƒçaju gre≈°ke\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Koristimo tqdm za prikaz napretka\n",
    "        futures = {executor.submit(augment_text, idx): idx for idx in sampled_indices}\n",
    "        for future in tqdm(as_completed(futures), total=num_augmented_instances, desc=f'Augmenting with {method.__name__}'):\n",
    "            augmented_texts.append(future.result())\n",
    "    \n",
    "    augmented_df = pd.DataFrame({text_column: augmented_texts})\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Augmenting with clare_augment:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:08:40,007 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "2024-11-15 19:08:40,109 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:369: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:369: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "Augmenting with clare_augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [9:19:27<00:00, 16.78s/it]  \n",
      "Augmenting with embedding_augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11000/11000 [14:15<00:00, 12.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originalni dataset: (24783, 3)\n",
      "Augmentisani dataset: (13000, 2)\n",
      "Konaƒçni dataset: (37783, 3)\n",
      "Augmentirani dataset je saƒçuvan u 'augmented_train.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "negative_class_df = data[data['label'] == 0]\n",
    "\n",
    "# Broj instanci po klasi\n",
    "num_class_0 = len(negative_class_df)\n",
    "num_class_1 = len(data[data['label'] == 1])\n",
    "\n",
    "# Izraƒçunajte razliku\n",
    "class_difference = abs(num_class_0 - num_class_1)\n",
    "\n",
    "# Broj metoda\n",
    "num_methods = len(textattack_methods)\n",
    "\n",
    "# Podelite negativnu klasu na delove po broju metoda\n",
    "split_dataframes = np.array_split(negative_class_df, num_methods)\n",
    "\n",
    "# Broj augmentisanih instanci po metodi\n",
    "num_instances_per_method = class_difference // num_methods\n",
    "\n",
    "augmented_dfs = []\n",
    "for method, split_df in zip(textattack_methods, split_dataframes):\n",
    "    if method==clare_augment:\n",
    "        num_instances_per_method=2000\n",
    "    else:\n",
    "        num_instances_per_method=11000\n",
    "    augmented_df = augment_dataset_parallel(\n",
    "        split_df, \n",
    "        text_column='clean_tweet', \n",
    "        method=method, \n",
    "        num_augmented_instances=num_instances_per_method\n",
    "    )\n",
    "    augmented_df['label'] = 0\n",
    "    augmented_dfs.append(augmented_df)\n",
    "\n",
    "final_augmented_df = pd.concat(augmented_dfs, ignore_index=True)\n",
    "\n",
    "# Kombinujte originalni i augmentisani dataset\n",
    "final = pd.concat([data, final_augmented_df], ignore_index=True)\n",
    "\n",
    "print(\"Originalni dataset:\", data.shape)\n",
    "print(\"Augmentisani dataset:\", final_augmented_df.shape)\n",
    "print(\"Konaƒçni dataset:\", final.shape)\n",
    "\n",
    "final.to_csv('augmented_train.csv', index=False, encoding='utf-8')\n",
    "print(\"Augmentirani dataset je saƒçuvan u 'augmented_train.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konaƒçni augmentirani dataset: (37783, 3)\n"
     ]
    }
   ],
   "source": [
    "augmented_df = pd.read_csv('augmented_train.csv')\n",
    "\n",
    "print(\"Konaƒçni augmentirani dataset:\", augmented_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\katarina.stanojkovic\\.cache\\huggingface\\hub\\models--gpt2-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Inicijalizacija GPT-2 modela i tokenizera\n",
    "gpt_model_name = 'gpt2-medium'  # Mo≈æete koristiti 'gpt2', 'gpt2-large', itd.\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n",
    "gpt_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    gpt_model.to('cuda')\n",
    "\n",
    "def gpt_augment(text, max_length=50, num_return_sequences=1, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generi≈°e augmentisani tekst koristeƒái GPT-2 model.\n",
    "\n",
    "    Argumenti:\n",
    "    - text: Originalni tekst za augmentaciju.\n",
    "    - max_length: Maksimalna du≈æina generisanog teksta.\n",
    "    - num_return_sequences: Broj generisanih sekvenci.\n",
    "    - temperature: Kontrola kreativnosti generisanog teksta.\n",
    "\n",
    "    Vraƒáa:\n",
    "    - Generisani tekst.\n",
    "    \"\"\"\n",
    "    input_ids = gpt_tokenizer.encode(text, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gpt_model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            eos_token_id=gpt_tokenizer.eos_token_id,\n",
    "            pad_token_id=gpt_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Uklonite originalni tekst iz generisanog teksta\n",
    "    augmented_text = generated_text[len(text):].strip()\n",
    "    return augmented_text if augmented_text else text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\katarina.stanojkovic\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Inicijalizacija T5 modela i tokenizera\n",
    "t5_model_name = 't5-small'  # Mo≈æete koristiti 't5-base', 't5-large', itd.\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "t5_model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    t5_model.to('cuda')\n",
    "\n",
    "def t5_paraphrase(text, max_length=50, num_return_sequences=1, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generi≈°e parafrazirani tekst koristeƒái T5 model.\n",
    "\n",
    "    Argumenti:\n",
    "    - text: Originalni tekst za parafraziranje.\n",
    "    - max_length: Maksimalna du≈æina generisanog teksta.\n",
    "    - num_return_sequences: Broj generisanih sekvenci.\n",
    "    - num_beams: Broj zrakova za beam search.\n",
    "\n",
    "    Vraƒáa:\n",
    "    - Lista parafraziranih tekstova.\n",
    "    \"\"\"\n",
    "    input_text = \"paraphrase: \" + text\n",
    "    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt', truncation=True)\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    paraphrased_texts = [t5_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return paraphrased_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broj negativnih instanci: 18593\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filtriranje negativne klase\n",
    "negative_class_df = augmented_df[augmented_df['label'] == 0]\n",
    "\n",
    "# Prikaz broja instanci u negativnoj klasi\n",
    "print(\"Broj negativnih instanci:\", negative_class_df.shape[0])\n",
    "\n",
    "# Definisanje broja instanci za augmentaciju\n",
    "# Na primer, augmentiramo 10.000 instanci\n",
    "num_augmented_instances = 5000\n",
    "\n",
    "# Ako je potrebno, odaberite nasumiƒçno podgrupu\n",
    "selected_df = negative_class_df.sample(n=num_augmented_instances, random_state=42)\n",
    "\n",
    "gpt_df = selected_df.sample(n=1000, random_state=42)\n",
    "t5_df = selected_df.drop(gpt_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primena GPT-based augmentacije...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Augmentation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [1:20:10<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Lista za skladi≈°tenje augmentisanih tekstova\n",
    "gpt_augmented_texts = []\n",
    "\n",
    "print(\"Primena GPT-based augmentacije...\")\n",
    "for text in tqdm(gpt_df['clean_tweet'], desc=\"GPT Augmentation\"):\n",
    "    augmented_text = gpt_augment(text)\n",
    "    gpt_augmented_texts.append(augmented_text)\n",
    "\n",
    "# Kreiranje DataFrame-a sa augmentiranim tekstovima\n",
    "gpt_augmented_df = pd.DataFrame({\n",
    "    'clean_tweet': gpt_augmented_texts,\n",
    "    'label': 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primena T5-based parafraziranja...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T5 Paraphrasing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [53:04<00:00,  1.26it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Lista za skladi≈°tenje augmentisanih tekstova\n",
    "t5_augmented_texts = []\n",
    "\n",
    "print(\"Primena T5-based parafraziranja...\")\n",
    "for text in tqdm(t5_df['clean_tweet'], desc=\"T5 Paraphrasing\"):\n",
    "    paraphrased_texts = t5_paraphrase(text)\n",
    "    # Odabir jednog parafraziranog teksta\n",
    "    augmented_text = paraphrased_texts[0] if paraphrased_texts else text\n",
    "    t5_augmented_texts.append(augmented_text)\n",
    "\n",
    "# Kreiranje DataFrame-a sa augmentiranim tekstovima\n",
    "t5_augmented_df = pd.DataFrame({\n",
    "    'clean_tweet': t5_augmented_texts,\n",
    "    'label': 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originalni dataset: (37783, 3)\n",
      "Dodatni augmentirani podaci: (5000, 2)\n",
      "Konaƒçni augmentirani dataset: (42783, 3)\n"
     ]
    }
   ],
   "source": [
    "# Kombinovanje GPT-based i T5-based augmentacija\n",
    "additional_augmented_df = pd.concat([gpt_augmented_df, t5_augmented_df], ignore_index=True)\n",
    "\n",
    "# Kombinovanje sa postojeƒáim augmentiranim dataset-om\n",
    "final_augmented_df = pd.concat([augmented_df, additional_augmented_df], ignore_index=True)\n",
    "\n",
    "# Prikaz veliƒçine finalnog augmentiranog dataset-a\n",
    "print(\"Originalni dataset:\", augmented_df.shape)\n",
    "print(\"Dodatni augmentirani podaci:\", additional_augmented_df.shape)\n",
    "print(\"Konaƒçni augmentirani dataset:\", final_augmented_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konaƒçni augmentirani dataset je saƒçuvan u 'fully_augmented_train.csv'\n"
     ]
    }
   ],
   "source": [
    "# ƒåuvanje konaƒçnog augmentiranog dataset-a\n",
    "final_augmented_df.to_csv('fully_augmented_train.csv', index=False, encoding='utf-8')\n",
    "print(\"Konaƒçni augmentirani dataset je saƒçuvan u 'fully_augmented_train.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
