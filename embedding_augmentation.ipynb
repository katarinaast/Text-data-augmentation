{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x == 1 else 0)\n",
    "df = df[['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column):\n",
    "    \"\"\"\n",
    "    Funkcija za predobradu teksta.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Pandas DataFrame koji sadr≈æi kolonu sa tekstom.\n",
    "    - text_column: Ime kolone koja sadr≈æi tekstualne podatke.\n",
    "    \n",
    "    Vraƒáa:\n",
    "    - df: Pandas DataFrame sa novom kolonom 'clean_text' koja sadr≈æi predobraƒëen tekst.\n",
    "    \"\"\"\n",
    "    # Kopiramo originalni DataFrame da ne bismo menjali originalne podatke\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Uklanjanje URL-ova i email adresa\n",
    "    df['clean_tweet'] = df[text_column].apply(lambda x: re.sub(r'http\\S+|www.\\S+|mailto:\\S+', '', x))\n",
    "    \n",
    "    # Uklanjanje HTML tagova\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "    \n",
    "    # Uklanjanje emotikona i specijalnih karaktera\n",
    "    def remove_emojis(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emotikoni\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # simobli i ikone\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport i simobli\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # zastave\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_emojis)\n",
    "    \n",
    "    # Uklanjanje specijalnih karaktera i interpunkcije\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^A-Za-z≈°ƒëƒçƒá≈æ≈†ƒêƒåƒÜ≈Ω ]+', ' ', x))\n",
    "    \n",
    "    # Pretvaranje u mala slova\n",
    "    df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "    \n",
    "    # Uklanjanje dijakritika\n",
    "    def remove_diacritics(text):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "        return text\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_diacritics)\n",
    "    \n",
    "    # Uklanjanje vi≈°estrukih razmaka\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
    "    \n",
    "    # Tokenizacija\n",
    "    df['tokens'] = df['clean_tweet'].apply(nltk.word_tokenize)\n",
    "    \n",
    "    # Uklanjanje stop-reƒçi\n",
    "    stop_words = set(stopwords.words('english'))  # Ako ima≈° stop-reƒçi za srpski, zameni ovde\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    \n",
    "    # Lematizacija\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # Spajanje tokena nazad u string\n",
    "    df['clean_tweet'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Uklanjanje nepotrebnih kolona\n",
    "    df = df.drop(columns=['tokens'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katarina.stanojkovic\\AppData\\Local\\Temp\\ipykernel_11304\\2996977020.py:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df['clean_tweet'] = df['clean_tweet'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_text(df, 'tweet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "from textattack.augmentation import Augmenter, BackTranslationAugmenter\n",
    "from textattack.transformations import WordInsertionMaskedLM, WordSwapWordNet\n",
    "from textattack.augmentation.recipes import CLAREAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\katarina.stanojkovic\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from textattack.augmentation import (\n",
    "    EmbeddingAugmenter,\n",
    "    CLAREAugmenter\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "# Inicijalizacija TextAttack augmentera\n",
    "embedding_augmenter = EmbeddingAugmenter()\n",
    "clare_augmenter = CLAREAugmenter(model='distilroberta-base', tokenizer='distilroberta-base')\n",
    "\n",
    "# Definisanje wrapper funkcija\n",
    "def embedding_augment(text, **kwargs):\n",
    "    augmented_texts = embedding_augmenter.augment(text)\n",
    "    return random.choice(augmented_texts) if augmented_texts else text\n",
    "\n",
    "def clare_augment(text, **kwargs):\n",
    "    augmented_texts = clare_augmenter.augment(text)\n",
    "    return random.choice(augmented_texts) if augmented_texts else text\n",
    "\n",
    "# Lista TextAttack augmentera kao funkcija\n",
    "textattack_methods = [\n",
    "    clare_augment,\n",
    "    embedding_augment\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "def augment_dataset_parallel(df, text_column, method, num_augmented_instances, max_workers=4, **kwargs):\n",
    "    \"\"\"\n",
    "    Primeni izabranu metodu augmentacije na dataset koristeƒái paralelizaciju.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Originalni DataFrame.\n",
    "    - text_column: Naziv kolone sa tekstom.\n",
    "    - method: Funkcija metode augmentacije.\n",
    "    - num_augmented_instances: Broj instanci koje treba generisati.\n",
    "    - max_workers: Broj paralelnih radnika.\n",
    "    - **kwargs: Dodatni argumenti za metodu augmentacije.\n",
    "    \n",
    "    Vraƒáa:\n",
    "    - DataFrame sa augmentiranim podacima.\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    indices = df.index.tolist()\n",
    "    num_samples = len(indices)\n",
    "    \n",
    "    # Ako je broj instanci veƒái od broja dostupnih uzoraka, uzmi uzorke sa zamjenom\n",
    "    replace = num_augmented_instances > num_samples\n",
    "    sampled_indices = np.random.choice(indices, size=num_augmented_instances, replace=replace)\n",
    "    \n",
    "    def augment_text(idx):\n",
    "        original_text = df.loc[idx, text_column]\n",
    "        try:\n",
    "            augmented_text = method(original_text, **kwargs)\n",
    "            return augmented_text\n",
    "        except Exception as e:\n",
    "            print(f\"Gre≈°ka pri augmentaciji teksta na indeksu {idx}: {e}\")\n",
    "            return original_text  # Vraƒáa originalni tekst u sluƒçaju gre≈°ke\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Koristimo tqdm za prikaz napretka\n",
    "        futures = {executor.submit(augment_text, idx): idx for idx in sampled_indices}\n",
    "        for future in tqdm(as_completed(futures), total=num_augmented_instances, desc=f'Augmenting with {method.__name__}'):\n",
    "            augmented_texts.append(future.result())\n",
    "    \n",
    "    augmented_df = pd.DataFrame({text_column: augmented_texts})\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Augmenting with clare_augment:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:08:40,007 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "2024-11-15 19:08:40,109 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:369: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:369: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From c:\\Users\\katarina.stanojkovic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "Augmenting with clare_augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [9:19:27<00:00, 16.78s/it]  \n",
      "Augmenting with embedding_augment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11000/11000 [14:15<00:00, 12.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originalni dataset: (24783, 3)\n",
      "Augmentisani dataset: (13000, 2)\n",
      "Konaƒçni dataset: (37783, 3)\n",
      "Augmentirani dataset je saƒçuvan u 'augmented_train.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "negative_class_df = data[data['label'] == 0]\n",
    "\n",
    "# Broj instanci po klasi\n",
    "num_class_0 = len(negative_class_df)\n",
    "num_class_1 = len(data[data['label'] == 1])\n",
    "\n",
    "# Izraƒçunajte razliku\n",
    "class_difference = abs(num_class_0 - num_class_1)\n",
    "\n",
    "# Broj metoda\n",
    "num_methods = len(textattack_methods)\n",
    "\n",
    "# Podelite negativnu klasu na delove po broju metoda\n",
    "split_dataframes = np.array_split(negative_class_df, num_methods)\n",
    "\n",
    "# Broj augmentisanih instanci po metodi\n",
    "num_instances_per_method = class_difference // num_methods\n",
    "\n",
    "augmented_dfs = []\n",
    "for method, split_df in zip(textattack_methods, split_dataframes):\n",
    "    if method==clare_augment:\n",
    "        num_instances_per_method=2000\n",
    "    else:\n",
    "        num_instances_per_method=11000\n",
    "    augmented_df = augment_dataset_parallel(\n",
    "        split_df, \n",
    "        text_column='clean_tweet', \n",
    "        method=method, \n",
    "        num_augmented_instances=num_instances_per_method\n",
    "    )\n",
    "    augmented_df['label'] = 0\n",
    "    augmented_dfs.append(augmented_df)\n",
    "\n",
    "final_augmented_df = pd.concat(augmented_dfs, ignore_index=True)\n",
    "\n",
    "# Kombinujte originalni i augmentisani dataset\n",
    "final = pd.concat([data, final_augmented_df], ignore_index=True)\n",
    "\n",
    "print(\"Originalni dataset:\", data.shape)\n",
    "print(\"Augmentisani dataset:\", final_augmented_df.shape)\n",
    "print(\"Konaƒçni dataset:\", final.shape)\n",
    "\n",
    "final.to_csv('augmented_train.csv', index=False, encoding='utf-8')\n",
    "print(\"Augmentirani dataset je saƒçuvan u 'augmented_train.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
