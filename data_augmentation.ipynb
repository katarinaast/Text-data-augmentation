{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preuzimanje potrebnih NLTK resursa\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Učitaj dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Prikaži prvih nekoliko redova\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Kreiraj novu kolonu 'label' za binarnu klasifikaciju\n",
    "# df['label'] = df['class'].apply(lambda x: 1 if x == 1 else 0)\n",
    "def plot_distribution(data, column):\n",
    "# Prikaži distribuciju klasa\n",
    "    class_counts = data[column].value_counts()\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "    plt.title('Distribucija klasa')\n",
    "    plt.xlabel('Klasa (0 = Nije govor mržnje, 1 = Govor mržnje)')\n",
    "    plt.ylabel('Broj primera')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column):\n",
    "    \"\"\"\n",
    "    Funkcija za predobradu teksta.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Pandas DataFrame koji sadrži kolonu sa tekstom.\n",
    "    - text_column: Ime kolone koja sadrži tekstualne podatke.\n",
    "    \n",
    "    Vraća:\n",
    "    - df: Pandas DataFrame sa novom kolonom 'clean_text' koja sadrži predobrađen tekst.\n",
    "    \"\"\"\n",
    "    # Kopiramo originalni DataFrame da ne bismo menjali originalne podatke\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Uklanjanje URL-ova i email adresa\n",
    "    df['clean_tweet'] = df[text_column].apply(lambda x: re.sub(r'http\\S+|www.\\S+|mailto:\\S+', '', x))\n",
    "    \n",
    "    # Uklanjanje HTML tagova\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "    \n",
    "    # Uklanjanje emotikona i specijalnih karaktera\n",
    "    def remove_emojis(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emotikoni\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # simobli i ikone\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport i simobli\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # zastave\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_emojis)\n",
    "    \n",
    "    # Uklanjanje specijalnih karaktera i interpunkcije\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^A-Za-zšđčćžŠĐČĆŽ ]+', ' ', x))\n",
    "    \n",
    "    # Pretvaranje u mala slova\n",
    "    df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "    \n",
    "    # Uklanjanje dijakritika\n",
    "    def remove_diacritics(text):\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "        return text\n",
    "\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(remove_diacritics)\n",
    "    \n",
    "    # Uklanjanje višestrukih razmaka\n",
    "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
    "    \n",
    "    # Tokenizacija\n",
    "    df['tokens'] = df['clean_tweet'].apply(nltk.word_tokenize)\n",
    "    \n",
    "    # Uklanjanje stop-reči\n",
    "    stop_words = set(stopwords.words('english'))  # Ako imaš stop-reči za srpski, zameni ovde\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    \n",
    "    # Lematizacija\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # Spajanje tokena nazad u string\n",
    "    df['clean_tweet'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Uklanjanje nepotrebnih kolona\n",
    "    df = df.drop(columns=['tokens'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_text(df, 'tweet')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Logistička regresija:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"SVM:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "def train_naive_bayes(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Naivni Bajes:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def prepare_tokenizer(texts, num_words=5000):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_pad(tokenizer, texts, maxlen=100):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=maxlen)\n",
    "    return padded\n",
    "\n",
    "def train_lstm(X_train, y_train, X_test, y_test, vocab_size, maxlen=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))\n",
    "    model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'LSTM Test Accuracy: {accuracy}')\n",
    "    return model\n",
    "\n",
    "def train_cnn(X_train, y_train, X_test, y_test, vocab_size, maxlen=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'CNN Test Accuracy: {accuracy}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_bert(X_train, y_train, X_test, y_test, model_name='bert-base-multilingual-cased', epochs=3):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenizacija i enkodiranje\n",
    "    def encode_texts(texts):\n",
    "        return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "    \n",
    "    train_encodings = encode_texts(X_train)\n",
    "    test_encodings = encode_texts(X_test)\n",
    "    \n",
    "    # Konvertovanje labela u tensor\n",
    "    y_train = tf.convert_to_tensor(y_train)\n",
    "    y_test = tf.convert_to_tensor(y_test)\n",
    "    \n",
    "    # Kreiranje TensorFlow dataset-a\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test))\n",
    "    \n",
    "    # Batchevi\n",
    "    train_dataset = train_dataset.shuffle(len(X_train)).batch(8)\n",
    "    test_dataset = test_dataset.batch(8)\n",
    "    \n",
    "    # Model\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # Kompajliranje modela\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "    \n",
    "    # Treniranje modela\n",
    "    model.fit(train_dataset, validation_data=test_dataset, epochs=epochs)\n",
    "    \n",
    "    # Evaluacija\n",
    "    loss, accuracy = model.evaluate(test_dataset)\n",
    "    print(f'BERT Test Accuracy: {accuracy}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_type='sklearn', tokenizer=None, maxlen=100):\n",
    "    \"\"\"\n",
    "    Evaluira model i prikazuje metrike performansi.\n",
    "    \n",
    "    Argumenti:\n",
    "    - model: istrenirani model\n",
    "    - X_test: test podaci (tekstualni ili vektorski)\n",
    "    - y_test: stvarne labela\n",
    "    - model_type: 'sklearn', 'keras', ili 'bert'\n",
    "    - tokenizer: Tokenizer objekat (potreban za neuronske mreže)\n",
    "    - maxlen: maksimalna dužina sekvenci (za neuronske mreže)\n",
    "    \"\"\"\n",
    "    if model_type == 'sklearn':\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif model_type == 'keras':\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    elif model_type == 'bert':\n",
    "        y_pred = model.predict(X_test).logits\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Nepoznat tip modela.\")\n",
    "    \n",
    "    print(\"Izveštaj o klasifikaciji:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Matrica konfuzije:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primena algoritama nad originalnim podacima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['clean_tweet']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4.4. Vektorizacija za klasične modele\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 4.7. Tokenizacija za neuronske mreže\n",
    "tokenizer = prepare_tokenizer(X_train, num_words=5000)\n",
    "X_train_seq = tokenize_and_pad(tokenizer, X_train)\n",
    "X_test_seq = tokenize_and_pad(tokenizer, X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 za rezervisani indeks 0\n",
    "\n",
    "# 4.12. Treniranje BERT modela\n",
    "# bert_model = train_bert(X_train.tolist(), y_train.tolist(), X_test.tolist(), y_test.tolist(), model_name='distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = train_logistic_regression(X_train_vec, y_train, X_test_vec, y_test)\n",
    "evaluate_model(lr_model, X_test_vec, y_test, model_type='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = train_svm(X_train_vec, y_train, X_test_vec, y_test)\n",
    "evaluate_model(svm_model, X_test_vec, y_test, model_type='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = train_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "evaluate_model(nb_model, X_test_vec, y_test, model_type='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = train_lstm(X_train_seq, y_train, X_test_seq, y_test, vocab_size)\n",
    "evaluate_model(lstm_model, X_test_seq, y_test, model_type='keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = train_cnn(X_train_seq, y_train, X_test_seq, y_test, vocab_size)\n",
    "evaluate_model(cnn_model, X_test_seq, y_test, model_type='keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentacija podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definisanje metoda za prve dve kombinacije\n",
    "# # methods_set1 = [synonym_replacement, random_insertion, back_translation_cached]\n",
    "# # methods_set2 = [simulate_typos, back_translation, bert_augmentation]\n",
    "\n",
    "# # OVO RADI\n",
    "# # FUNKCIJE KOJE KORISTE LOCKING GA BLOKIRAJU\n",
    "# # Ciljani broj instanci u negativnoj klasi (balansiranje)\n",
    "# positive_count = data[data['label'] == 1].shape[0]\n",
    "# print(f\"Broj pozitivnih instanci (label=1): {positive_count}\")\n",
    "\n",
    "# # Treniranje augmentacije na set1\n",
    "# print(\"\\nAugmentacija - Set 1:\")\n",
    "# df_augmented_set1_parallel = augment_negative_class_parallel(\n",
    "#     data, \n",
    "#     text_column='clean_tweet', \n",
    "#     label_column='label', \n",
    "#     methods=[back_translation_cached, random_character_swap], \n",
    "#     target_count=positive_count, \n",
    "#     max_workers=8  # Podesi broj radnika prema dostupnim resursima\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importovanje potrebnih biblioteka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kaca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kaca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Kaca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from googletrans import Translator\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preuzimanje potrebnih NLTK resursa\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definisanje metoda augmentacije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metode na nivou karaktera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_spelling_errors(text, error_prob=0.1):\n",
    "    \"\"\"\n",
    "    Simulira pravopisne greške u tekstu.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - error_prob: Verovatnoća da će svaki karakter biti izmenjen.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa simuliranim pravopisnim greškama.\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        if c.lower() in letters and random.random() < error_prob:\n",
    "            result += random.choice(letters)\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyboard_augmenter(text, error_prob=0.1):\n",
    "    \"\"\"\n",
    "    Simulira greške u kucanju na osnovu rasporeda tastature.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - error_prob: Verovatnoća da će svaki karakter biti zamenjen.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa simuliranim greškama u kucanju.\n",
    "    \"\"\"\n",
    "    keyboard_neighbors = {\n",
    "        'a': 'qwsz',\n",
    "        'b': 'vghn',\n",
    "        'c': 'xdfv',\n",
    "        'd': 'ersfcx',\n",
    "        'e': 'wsdfr',\n",
    "        'f': 'rtgdvc',\n",
    "        'g': 'tyfhvb',\n",
    "        'h': 'yugjbn',\n",
    "        'i': 'ujklo',\n",
    "        'j': 'uikhmn',\n",
    "        'k': 'ijolm,',\n",
    "        'l': 'kop;.',\n",
    "        'm': 'njk,',\n",
    "        'n': 'bhjm',\n",
    "        'o': 'iklp',\n",
    "        'p': 'ol;',\n",
    "        'q': 'wa',\n",
    "        'r': 'edft',\n",
    "        's': 'wedxz',\n",
    "        't': 'rfgy',\n",
    "        'u': 'yhji',\n",
    "        'v': 'cfgb',\n",
    "        'w': 'qase',\n",
    "        'x': 'zsdc',\n",
    "        'y': 'tghu',\n",
    "        'z': 'asx'\n",
    "    }\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        lower_c = c.lower()\n",
    "        if lower_c in keyboard_neighbors and random.random() < error_prob:\n",
    "            result += random.choice(keyboard_neighbors[lower_c])\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_simulation(text, error_prob=0.05):\n",
    "    \"\"\"\n",
    "    Simulira OCR greške u tekstu.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - error_prob: Verovatnoća da će svaki karakter biti zamenjen.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa simuliranim OCR greškama.\n",
    "    \"\"\"\n",
    "    ocr_errors = {\n",
    "        '0': ['O', 'o'],\n",
    "        '1': ['I', 'l'],\n",
    "        'l': ['1', 'I'],\n",
    "        'O': ['0', 'o'],\n",
    "        'o': ['0', 'O'],\n",
    "        'I': ['1', 'l']\n",
    "    }\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        if c in ocr_errors and random.random() < error_prob:\n",
    "            result += random.choice(ocr_errors[c])\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_character_augmentation(text, aug_prob=0.1):\n",
    "    \"\"\"\n",
    "    Nasumična augmentacija karaktera u tekstu.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - aug_prob: Verovatnoća da će svaka operacija biti primenjena na karakter.\n",
    "    \n",
    "    Vraća:\n",
    "    - Augmentirani tekst.\n",
    "    \"\"\"\n",
    "    functions = [delete_random_char, substitute_random_char, swap_random_chars, insert_random_char]\n",
    "    augmented_text = text\n",
    "    for func in functions:\n",
    "        augmented_text = func(augmented_text, aug_prob)\n",
    "    return augmented_text\n",
    "\n",
    "def delete_random_char(text, aug_prob):\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        if random.random() > aug_prob:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def substitute_random_char(text, aug_prob):\n",
    "    letters = string.ascii_letters\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        if random.random() < aug_prob:\n",
    "            result += random.choice(letters)\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def swap_random_chars(text, aug_prob):\n",
    "    text = list(text)\n",
    "    for i in range(len(text)-1):\n",
    "        if random.random() < aug_prob:\n",
    "            text[i], text[i+1] = text[i+1], text[i]\n",
    "    return ''.join(text)\n",
    "\n",
    "def insert_random_char(text, aug_prob):\n",
    "    result = ''\n",
    "    letters = string.ascii_letters\n",
    "    for c in text:\n",
    "        result += c\n",
    "        if random.random() < aug_prob:\n",
    "            result += random.choice(letters)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metode na nivou reci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(text, n=2):\n",
    "    \"\"\"\n",
    "    Zamenjuje n reči njihovim sinonimima.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj reči koje će biti zamenjene.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa zamenjenim sinonimima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    augmented_text = ' '.join(new_words)\n",
    "    return augmented_text\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            synonym = lem.name().replace('_', ' ').lower()\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_replacement(text, n=2):\n",
    "    \"\"\"\n",
    "    Zamenjuje n reči njihovim antonimima.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj reči koje će biti zamenjene.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa zamenjenim antonimima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        antonyms = get_antonyms(random_word)\n",
    "        if len(antonyms) >= 1:\n",
    "            antonym = random.choice(antonyms)\n",
    "            new_words = [antonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    augmented_text = ' '.join(new_words)\n",
    "    return augmented_text\n",
    "\n",
    "def get_antonyms(word):\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            for ant in lem.antonyms():\n",
    "                antonym = ant.name().replace('_', ' ').lower()\n",
    "                antonyms.add(antonym)\n",
    "    return list(antonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insertion(text, n=2):\n",
    "    \"\"\"\n",
    "    Umeće n sinonima nasumično u tekst.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj reči koje će biti umetnute.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa umetnutim rečima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        add_word(words)\n",
    "    augmented_text = ' '.join(words)\n",
    "    return augmented_text\n",
    "\n",
    "def add_word(words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1 and counter < 10:\n",
    "        random_word = words[random.randint(0, len(words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "    if len(synonyms) >= 1:\n",
    "        synonym = random.choice(synonyms)\n",
    "        random_idx = random.randint(0, len(words))\n",
    "        words.insert(random_idx, synonym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(text, n=2):\n",
    "    \"\"\"\n",
    "    Menja mesta dvema rečima u tekstu n puta.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj zamena.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa zamenjenim rečima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        words = swap_word(words)\n",
    "    augmented_text = ' '.join(words)\n",
    "    return augmented_text\n",
    "\n",
    "def swap_word(words):\n",
    "    idx1 = random.randint(0, len(words)-1)\n",
    "    idx2 = random.randint(0, len(words)-1)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(text, p=0.2):\n",
    "    \"\"\"\n",
    "    Briše reči iz teksta sa verovatnoćom p.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - p: Verovatnoća brisanja reči.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa obrisanim rečima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if random.uniform(0,1) > p:\n",
    "            new_words.append(word)\n",
    "    if len(new_words) == 0:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_augmentation(text, n=2):\n",
    "    \"\"\"\n",
    "    Deli n reči na manje delove.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj reči koje će biti podeljene.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa podeljenim rečima.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    num_splits = 0\n",
    "    while num_splits < n:\n",
    "        idx = random.randint(0, len(new_words)-1)\n",
    "        word = new_words[idx]\n",
    "        if len(word) > 1:\n",
    "            split_point = random.randint(1, len(word)-1)\n",
    "            new_words[idx:idx+1] = [word[:split_point], word[split_point:]]\n",
    "            num_splits += 1\n",
    "        if num_splits >= n:\n",
    "            break\n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_augmentation(text, error_prob=0.1):\n",
    "    \"\"\"\n",
    "    Umeće pravopisne greške u reči.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - error_prob: Verovatnoća da će reč biti izmenjena.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa pravopisnim greškama.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if random.random() < error_prob:\n",
    "            new_word = random_character_augmentation(word, aug_prob=0.2)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Učitajte pretrenirane rečničke vektore (npr. GloVe ili Word2Vec)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ovaj primer koristi Google pretrenirani Word2Vec model (mora biti preuzet)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membedding_replacement\u001b[39m(text, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, word_vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Učitajte pretrenirane rečničke vektore (npr. GloVe ili Word2Vec)\n",
    "# Ovaj primer koristi Google pretrenirani Word2Vec model (mora biti preuzet)\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def embedding_replacement(text, n=2, word_vectors=None):\n",
    "    \"\"\"\n",
    "    Zamenjuje reči sličnim rečima na osnovu embeddinga.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - n: Broj reči koje će biti zamenjene.\n",
    "    - word_vectors: Pretrenirani model rečničkih vektora.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa zamenjenim rečima.\n",
    "    \"\"\"\n",
    "    if word_vectors is None:\n",
    "        raise ValueError(\"word_vectors model must be provided\")\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for word in random_word_list:\n",
    "        if word in word_vectors:\n",
    "            similar_words = [w for w, sim in word_vectors.most_similar(word)]\n",
    "            if similar_words:\n",
    "                similar_word = random.choice(similar_words)\n",
    "                new_words = [similar_word if w == word else w for w in new_words]\n",
    "                num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_embedding_augmentation(text, mask_token='[MASK]', top_k=5):\n",
    "    \"\"\"\n",
    "    Zamenjuje reči koristeći kontekstualne modele (npr. BERT).\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - mask_token: Token koji se koristi za maskiranje (default: '[MASK]').\n",
    "    - top_k: Broj najboljih predikcija za izbor.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa zamenjenim rečima.\n",
    "    \"\"\"\n",
    "    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    words = text.split()\n",
    "    num_masks = max(1, int(0.15 * len(words)))\n",
    "    masked_indices = random.sample(range(len(words)), num_masks)\n",
    "    for idx in masked_indices:\n",
    "        original_word = words[idx]\n",
    "        words[idx] = mask_token\n",
    "        masked_text = ' '.join(words)\n",
    "        predictions = unmasker(masked_text)\n",
    "        for pred in predictions[:top_k]:\n",
    "            new_word = pred['token_str']\n",
    "            if new_word != original_word:\n",
    "                words[idx] = new_word\n",
    "                break\n",
    "        else:\n",
    "            words[idx] = original_word\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metode na nivu fraza i recenica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_text(text, model=None, tokenizer=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Parafrazira tekst koristeći T5 model.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - model: Pretrenirani T5 model.\n",
    "    - tokenizer: T5 tokenizer.\n",
    "    - device: 'cpu' ili 'cuda'.\n",
    "    \n",
    "    Vraća:\n",
    "    - Parafrazirani tekst.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "        tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "        model = model.to(device)\n",
    "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
    "    encoding = tokenizer.encode_plus(input_text, padding='longest', return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, attention_mask=attention_masks,\n",
    "        max_length=256,\n",
    "        do_sample=True,\n",
    "        top_k=120,\n",
    "        top_p=0.95,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translation(text, src_language='en', target_language='de'):\n",
    "    \"\"\"\n",
    "    Back-Translation metode.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    - src_language: Izvorni jezik.\n",
    "    - target_language: Ciljni jezik za prevođenje.\n",
    "    \n",
    "    Vraća:\n",
    "    - Back-translated tekst.\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(text, src=src_language, dest=target_language).text\n",
    "    back_translated = translator.translate(translated, src=target_language, dest=src_language).text\n",
    "    return back_translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sentence_augmentation(text):\n",
    "    \"\"\"\n",
    "    Nasumično menja redosled rečenica u tekstu.\n",
    "    \n",
    "    Argumenti:\n",
    "    - text: Originalni tekst.\n",
    "    \n",
    "    Vraća:\n",
    "    - Tekst sa promenjenim redosledom rečenica.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    random.shuffle(sentences)\n",
    "    return ' '.join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_gpt2(prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Generiše tekst koristeći GPT-2 model.\n",
    "    \n",
    "    Argumenti:\n",
    "    - prompt: Početni tekst.\n",
    "    - max_length: Maksimalna dužina generisanog teksta.\n",
    "    \n",
    "    Vraća:\n",
    "    - Generisani tekst.\n",
    "    \"\"\"\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    outputs = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    return outputs[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(df, text_column, method, num_augmented_instances, **kwargs):\n",
    "    \"\"\"\n",
    "    Primeni izabranu metodu augmentacije na dataset.\n",
    "    \n",
    "    Argumenti:\n",
    "    - df: Originalni DataFrame.\n",
    "    - text_column: Naziv kolone sa tekstom.\n",
    "    - method: Funkcija metode augmentacije.\n",
    "    - num_augmented_instances: Broj instanci koje treba generisati.\n",
    "    - **kwargs: Dodatni argumenti za metodu augmentacije.\n",
    "    \n",
    "    Vraća:\n",
    "    - DataFrame sa augmentiranim podacima.\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    indices = df.index.tolist()\n",
    "    num_samples = len(indices)\n",
    "    \n",
    "    # Ako je broj instanci veći od broja dostupnih uzoraka, uzmi uzorke sa zamjenom\n",
    "    replace = num_augmented_instances > num_samples\n",
    "    sampled_indices = np.random.choice(indices, size=num_augmented_instances, replace=replace)\n",
    "    \n",
    "    for idx in tqdm(sampled_indices, desc=f'Augmenting with {method.__name__}'):\n",
    "        original_text = df.loc[idx, text_column]\n",
    "        augmented_text = method(original_text, **kwargs)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    augmented_df = pd.DataFrame({text_column: augmented_texts})\n",
    "    return augmented_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting with synonym_replacement: 100%|██████████| 20/20 [00:00<00:00, 705.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Definiši broj instanci koje želiš da generišeš\n",
    "num_instances = 20\n",
    "methods = [simulate_spelling_errors, keyboard_augmenter, ocr_simulation, random_character_augmentation, synonym_replacement, antonym_replacement, random_insertion, random_swap, random_deletion, split_augmentation, spelling_augmentation, back_translation, random_sentence_augmentation]\n",
    "\n",
    "augmented_df_synonyms = augment_dataset(\n",
    "    data, \n",
    "    text_column='clean_tweet', \n",
    "    method=synonym_replacement, \n",
    "    num_augmented_instances=num_instances, \n",
    "    n=2  # Broj reči koje će biti zamenjene\n",
    ")\n",
    "\n",
    "# for method in methods:\n",
    "#     augmented_df = augment_dataset(\n",
    "#     data, \n",
    "#     text_column='clean_tweet', \n",
    "#     method=method, \n",
    "#     num_augmented_instances=num_instances\n",
    "#             )\n",
    "#     print(\"Method: \", method)\n",
    "#     print(augmented_df, '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_set1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predobrada nije potrebna jer smo već očistili tekst\n",
    "\n",
    "# Podela na trening i test skup\n",
    "X_aug1 = df_augmented_set1['clean_tweet']\n",
    "y_aug1 = df_augmented_set1['label']\n",
    "\n",
    "X_train_aug1, X_test_aug1, y_train_aug1, y_test_aug1 = train_test_split(X_aug1, y_aug1, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_aug1 = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec_aug1 = vectorizer_aug1.fit_transform(X_train_aug1)\n",
    "X_test_vec_aug1 = vectorizer_aug1.transform(X_test_aug1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistička regresija\n",
    "lr_model_aug1 = train_logistic_regression(X_train_vec_aug1, y_train_aug1, X_test_vec_aug1, y_test_aug1)\n",
    "\n",
    "# SVM\n",
    "svm_model_aug1 = train_svm(X_train_vec_aug1, y_train_aug1, X_test_vec_aug1, y_test_aug1)\n",
    "\n",
    "# Naivni Bajes\n",
    "nb_model_aug1 = train_naive_bayes(X_train_vec_aug1, y_train_aug1, X_test_vec_aug1, y_test_aug1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizacija\n",
    "tokenizer_aug1 = prepare_tokenizer(X_train_aug1, num_words=5000)\n",
    "X_train_seq_aug1 = tokenize_and_pad(tokenizer_aug1, X_train_aug1)\n",
    "X_test_seq_aug1 = tokenize_and_pad(tokenizer_aug1, X_test_aug1)\n",
    "vocab_size_aug1 = len(tokenizer_aug1.word_index) + 1\n",
    "\n",
    "# LSTM\n",
    "lstm_model_aug1 = train_lstm(X_train_seq_aug1, y_train_aug1, X_test_seq_aug1, y_test_aug1, vocab_size_aug1)\n",
    "\n",
    "# CNN\n",
    "cnn_model_aug1 = train_cnn(X_train_seq_aug1, y_train_aug1, X_test_seq_aug1, y_test_aug1, vocab_size_aug1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
